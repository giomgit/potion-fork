[DEFAULT]
path                = results_lq_s5
repetitions         = 30
iterations          = 100

# Policy
mu_init             = 0.0
logstd_init         = 0.0
learn_std           = False
action_filter       = None

# Environment
environment         = 'lq'
horizon             = 10
sigma_noise         = 0
state_dim           = 5

# Common algorithm parameters
baseline            = 'avg'
estimator           = 'gpomdp'
seed                = 42
stepper             = 'ConstantStepper(1e-4)'

# Offpolicy algorithm parameters
biased_offpolicy    = True
ce_batchsizes       = None
ce_use_offline_data = True
defensive           = True


[onpolicy]
experiment          = grid
ce_batchsizes       = None
logstd_init         = [-1.0, -2.0, -3.0]


[offpolicy]
experiment          = grid
ce_batchsizes       = 500
batchsize           = 500
logstd_init         = [-1.0, -2.0, -3.0]
