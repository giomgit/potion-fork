[DEFAULT]
path = swimmer/results/offpolicy
repetitions = 1
iterations = 400
log_ce_params_norms = True
log_params_norms = True

# Policy
mu_init = None
logstd_init = 0.0
learn_std = False

# Environment
environment = 'swimmer'
horizon = 100
gamma = 0.99

# Common algorithm parameters
baseline = 'avg'
estimator = 'gpomdp'
seed = 42
stepper = 'ConstantStepper(1e-5)'
# stepper = 'Adam(1e-3)'

# Offpolicy algorithm parameters
batchsize = 100
biased_offpolicy = False
ce_batchsizes = None
ce_initialize_behavioural_policies = 'target'
ce_use_offline_data = True
ce_tol_grad = 1e3
ce_lr = 1e-4
ce_max_iter = 1e3
ce_mis_rescale = False
ce_mis_normalize = False
ce_mis_clip = None
ce_optimizer = 'adam'
ce_weight_decay = 10
ce_optimize_mean = True
ce_optimize_variance = True
defensive_batch = 0

[offpolicy_616]
experiment = grid
batchsize = 100
ce_mis_rescale = [False, True]
defensive_batch = [0, 50]

[offpolicy_619]
experiment = grid
batchsize = 50
ce_mis_rescale = True
defensive_batch = [0, 20, 50]
ce_initialize_behavioural_policies = ['target', 'reset']

[offpolicy_622]
batchsize = 50
ce_mis_rescale = True
ce_tol_grad = 10

[offpolicy_623]
batchsize = 50
ce_batchsizes = '[50]'
ce_use_offline_data = False

[offpolicy_628]
experiment = grid
batchsize = 50
ce_batchsizes = '[50]'
ce_use_offline_data = False
ce_optimize_mean = [True, False]

# experiment = grid
# batchsize = 50
# ce_batchsizes = '[50]'
# ce_use_offline_data = False
[offpolicy_630]
batchsize = 50
ce_batchsizes = '[50]'
ce_use_offline_data = False

[offpolicy_706]
# 100 batch
# CE solo varianza, ma diversa tolleranza gradiente
batchsize = 50
ce_batchsizes = '[50]'
ce_use_offline_data = False
ce_optimize_mean = False
ce_optimize_variance = True
ce_tol_grad = 10

[offpolicy_707]
# 50 batch
# CE media e varianza
batchsize = 50
ce_initialize_behavioural_policies = 'target'

[offpolicy_707_debug]
# 50 batch
# CE media e varianza
batchsize = 50
ce_initialize_behavioural_policies = 'target'
