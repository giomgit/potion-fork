[DEFAULT]
path = swimmer/results
repetitions = 1
iterations = 200

# Policy
mu_init = None
logstd_init = 0.0
learn_std = False

# Environment
environment = 'swimmer'
horizon = 500
gamma = 0.995

# Common algorithm parameters
baseline = 'avg'
estimator = 'gpomdp'
seed = 42
stepper = 'Adam(1e-3)'

# Offpolicy algorithm parameters
biased_offpolicy = True
ce_batchsizes = None
ce_use_offline_data = True
ce_tol_grad = 1e3
ce_lr = 1e-5
ce_max_iter = 1e4
ce_optimizer = 'adam'

[onpolicy]
experiment = list
batchsize = [10, 20, 40]

[offpolicy]
experiment = grid
batchsize = [10, 20, 40]
defensive_batch = [ 0, 0, 0]
tol_grad = [10]

[offpolicy_debug]
experiment = grid
batchsize = [10, 30]
defensive_batch = [0, 10]
ce_weight_decay = [10, 100, 1000]
