[DEFAULT]
path                = swimmer/results
repetitions         = 1
iterations          = 200

# Policy
mu_init             = 0.0
logstd_init         = 0.0
learn_std           = False

# Environment
environment         = 'swimmer'
horizon             = 500
gamma               = 0.995

# Common algorithm parameters
baseline            = 'avg'
estimator           = 'gpomdp'
seed                = 42
stepper             = 'Adam(1e-3)'

# Offpolicy algorithm parameters
biased_offpolicy    = True
ce_batchsizes       = None
ce_use_offline_data = True

[onpolicy]
experiment          = grid
batchsize           = [10, 20, 40]
