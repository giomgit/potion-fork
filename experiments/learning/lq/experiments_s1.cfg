[DEFAULT]
path                = lq/results_s1_1118
repetitions         = 30
iterations          = 100

# Policy
mu_init             = 0.0
logstd_init         = 0.0
learn_std           = False

# Environment
environment         = 'lq'
state_dim           = 1
sigma_noise         = 0
horizon             = 10

# Common algorithm parameters
baseline            = 'avg'
estimator           = 'gpomdp'
seed                = 42
stepper             = 'ConstantStepper(1e-4)'

# Offpolicy algorithm parameters
biased_offpolicy    = True
ce_batchsizes       = None
ce_use_offline_data = True
ce_initialize_behavioural_policies = 'target'
ce_tol_grad = 1e3
ce_lr = 1e-4
ce_max_iter = 1e3
ce_mis_normalize = False
ce_mis_clip = None
ce_optimizer = 'adam'
ce_weight_decay = 10
ce_optimize_mean = True
ce_optimize_variance = True
defensive_batch = 0

[onpolicy]
batchsize = 100

[offpolicy]
experiment = list
batchsize = [70, 70, 50, 50]
defensive_batch = [0, 30, 20, 40]