[DEFAULT]
path = cartpole/results/paper
repetitions = 10
iterations = 1

# Policy
mu_init = 0.0
logstd_init = 0.0
learn_std = False

# Environment
environment = 'cartpole'
sigma_noise = 0
horizon = 200

# Common algorithm parameters
baseline = 'peters'
estimator = 'gpomdp'
seed = 42
stepper = 'ConstantStepper(1e-2)'
stormpg_iterations = 191 # To have the same total number of trajectories compared with onpolicy and offpolicy

[stormpg]
experiment = list
init_batchsize = [50, 100, 200, 500, 1000]
mini_batchsize = [5, 10, 20, 50, 100]
