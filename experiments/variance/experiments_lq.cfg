[DEFAULT]
env                 = lq
path                = results_lq
repetitions         = 100
iterations          = 1

seed                = 42
state_dim           = 1
action_filter       = None
horizon             = 1
mu_init             = 0.0
logstd_init         = 0.0
estimator           = gpomdp
baseline            = peters
learn_std           = False
ce_divergence       = 'kl'
ce_lr               = 1-e5
ce_max_iter         = 1e5
ce_tol_grad         = 1e-1
ce_batchsizes       = "[200]"
batchsize           = 200
defensive           = True
biased_offpolicy    = True

[means]
experiment = grid
mu_init = [-1.0, -0.5, 0.0, 0.5, 1.0]

[stds]
experiment = grid
logstd_init = [-1.0, -0.5, 0.0, 0.5, 1.0]

[horizons]
experiment = grid
horizon = [1,2,5,10]

[dimensions]
experiment = grid
state_dim = [2, 5, 10]