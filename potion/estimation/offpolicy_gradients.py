#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Policy gradient stimators
@author: Matteo Papini
"""

import functools
import math
import torch

import potion.common.torch_utils as tu
from potion.common.misc_utils import unpack, discount
from potion.common.torch_utils import tensormat, jacobian
from potion.estimation.moments import incr_mean, incr_var


def off_gpomdp_estimator(batch, disc, policy, target_params, 
                         baselinekind='avg', 
                         result='mean',
                         shallow=False):
    """G(PO)MDP policy gradient estimator
       
    batch: list of N trajectories generated by behavioral policy. Each trajectory is a tuple 
        (states, actions, rewards, mask). Each element of the tuple is a 
        tensor where the first dimension is time.
    disc: discount factor
    policy: the one used to collect the data
    target_params: parameters of the policy to evaluate
    baselinekind: kind of baseline to employ in the estimator. 
        Either 'avg' (average reward, default), 'peters' 
        (variance-minimizing),  or 'zero' (no baseline)
    result: whether to return the final estimate ('mean', default), or the 
        single per-trajectory estimates ('samples')
    shallow: whether to use precomputed score functions (only available
        for shallow policies)
    """
    if shallow:
        return _shallow_off_gpomdp_estimator(batch, disc, policy, target_params, baselinekind, result)
    else:
        raise NotImplementedError

"From Mastrangelo's master thesis"
def _shallow_off_gpomdp_estimator(batch, disc, policy, target_params, 
                                  baselinekind='peters', 
                                  result='mean'):
    with torch.no_grad():
        states, actions, rewards, mask, _ = unpack(batch) # NxHxm, NxHxd, NxH, NxH
        disc_rewards = discount(rewards, disc) #NxH

        
        behavioral_params = policy.get_flat()
        policy.set_from_flat(target_params)
        scores = policy.score(states, actions) #NxHxD
        G = torch.cumsum(tensormat(scores, mask), 1) #NxHxD

        target_logps = policy.log_pdf(states, actions) #NxH
        policy.set_from_flat(behavioral_params)
        behavioral_logps = policy.log_pdf(states, actions) #NxH
        log_iws = torch.cumsum(target_logps - behavioral_logps, 1) #NxH
        stabilizers, _ = torch.max(log_iws, dim=1, keepdim=True) #NxH
        
        if baselinekind == 'peters':
            baseline = torch.sum(tensormat(G ** 2, disc_rewards * torch.exp(2*(log_iws - stabilizers))), 0) / \
                            torch.sum(tensormat(G ** 2, torch.exp(2*(log_iws - stabilizers))), 0) #HxD
        elif baselinekind == 'avg':
            baseline = torch.mean(disc_rewards, 0).unsqueeze    #[H,1]
        elif baselinekind == 'zero':
            baseline = torch.zeros_like(G[0]) #HxD
        else:
            raise NotImplementedError
        baseline[baseline != baseline] = 0 #removes non-real values

        values = disc_rewards.unsqueeze(2) - baseline.unsqueeze(0) #NxHxD
        
        _samples = torch.sum(tensormat(G * values, mask * torch.exp(log_iws)), 1) #NxD
        if result == 'samples':
            return _samples #NxD
        else:
            return torch.mean(_samples, 0) #D
    
def _shallow_multioff_gpomdp_estimator(batch, disc, target_policy, behavioural_policies, alphas,*,
                                  baselinekind='peters', 
                                  result='mean'):
    # Check parameters
    # ----------------
    if not isinstance(behavioural_policies,list): 
        behavioural_policies = [behavioural_policies]
    if not isinstance(alphas,list): 
        alphas = [alphas]

    assert len(behavioural_policies)==len(alphas), (
        "Parameters proposal_policies and alphas do not have same lenght")
    assert abs(sum(alphas) - 1) <= 1e-5, (
        "Parameter alphas do not sum to 1")

    # Estimate gradients
    # ------------------
    
    # Utlity function
    # Usage: log(x+y) = smoothmax(log(x),log(y)) := log(x) + log(1+exp(log(y)-log(x)))
    smoothmax = lambda x,y: x + torch.log(1+torch.exp(y-x))

    with torch.no_grad():
        # Samples
        states, actions, rewards, mask, _   = unpack(batch) # [N,H,S], [N,H,A], [N,H], [N,H]

        # Scores
        disc_rewards    = discount(rewards, disc)                       # [N,H]
        _scores         = target_policy.score(states, actions)          # [N,H,D]
        scores          = torch.cumsum(tensormat(_scores, mask), 1)     # [N,H,D]

        # Importance weights
        _target_logps       = target_policy.log_pdf(states, actions)*mask       # [N,H]
        target_logps_cms    = torch.cumsum(_target_logps, 1)                    # [N,H]

        _behavioral_logps_cms = [None]*len(behavioural_policies)
        for i,p in enumerate(behavioural_policies):
            _behavioral_logps_cms[i] = math.log(alphas[i]) + torch.cumsum(p.log_pdf(states, actions)*mask, 1)   # [N,H]
        behavioral_logps_cms = functools.reduce(smoothmax, _behavioral_logps_cms) # [N,H]
        
        log_iws             = target_logps_cms - behavioral_logps_cms           # [N,H]
        stabilizers, _      = torch.max(log_iws, dim=1, keepdim=True)           # [N,H]

        # Baseline
        H = scores.shape[1]
        D = scores.shape[2]
        if baselinekind == 'peters':
            baseline = torch.sum(tensormat(scores ** 2, disc_rewards * torch.exp(2*(log_iws - stabilizers))), 0) / \
                            torch.sum(tensormat(scores ** 2, torch.exp(2*(log_iws - stabilizers))), 0)  # [H,D]
        elif baselinekind == 'avg':
            baseline = torch.mean(disc_rewards, 0)          #[H]
            baseline = baseline.reshape(H,1).repeat(1,D)    #[H,D]
        elif baselinekind == 'zero':
            baseline = torch.zeros_like(scores[0]) # [H,D]
        else:
            raise NotImplementedError
        baseline[baseline != baseline] = 0 #removes non-real values
        values = disc_rewards.unsqueeze(2) - baseline.unsqueeze(0) # [N,H,D]

        # Gradient samples
        grad_samples = torch.sum(tensormat(scores * values, mask * torch.exp(log_iws)), 1) # [N,D]
        if result == 'samples':
            return grad_samples # [N,D]
        else:
            return torch.mean(grad_samples, 0) # [D]

#entropy-augmented version
def egpomdp_estimator(batch, disc, policy, coeff, baselinekind='avg', result='mean',
                     shallow=False):
    """G(PO)MDP policy gradient estimator
       
    batch: list of N trajectories. Each trajectory is a tuple 
        (states, actions, rewards, mask). Each element of the tuple is a 
        tensor where the first dimension is time.
    disc: discount factor
    policy: the one used to collect the data
    coeff: entropy bonus coefficient
    baselinekind: kind of baseline to employ in the estimator. 
        Either 'avg' (average reward, default), 'peters' 
        (variance-minimizing),  or 'zero' (no baseline)
    result: whether to return the final estimate ('mean', default), or the 
        single per-trajectory estimates ('samples')
    shallow: whether to use precomputed score functions (only available
        for shallow policies)
    """
    if shallow:
        return _shallow_egpomdp_estimator(batch, disc, policy, coeff, baselinekind, result)    
    else:
        raise NotImplementedError


def reinforce_estimator(batch, disc, policy, baselinekind='avg', 
                        result='mean', shallow=False):
    """REINFORCE policy gradient estimator
       
    batch: list of N trajectories. Each trajectory is a tuple 
        (states, actions, rewards, mask). Each element of the tuple is a 
        tensor where the first dimension is time.
    disc: discount factor
    policy: the one used to collect the data
    baselinekind: kind of baseline to employ in the estimator. 
        Either 'avg' (average reward, default), 'peters' 
        (variance-minimizing),  or 'zero' (no baseline)
    result: whether to return the final estimate ('mean', default), or the 
        single per-trajectory estimates ('samples')
    shallow: whether to use precomputed score functions (only available
        for shallow policies)
    """
    raise NotImplementedError


#entopy-augmented version     
def _shallow_egpomdp_estimator(batch, disc, policy, coeff, baselinekind='peters', result='mean'):
    raise NotImplementedError

        
def _shallow_reinforce_estimator(batch, disc, policy, baselinekind='peters', result='mean'):
    raise NotImplementedError


def _incr_shallow_gpomdp_estimator(traj, disc, policy, baselinekind='peters', result='mean', cum_1 = 0., cum_2 = 0., cum_3 = 0., tot_trajs = 1):
    raise NotImplementedError

        
"""Testing"""
if __name__ == '__main__':
    import gym.spaces

    import potion.envs
    from potion.actors.continuous_policies import ShallowGaussianPolicy as Gauss
    from potion.simulation.trajectory_generators import generate_batch
    from potion.common.misc_utils import seed_all_agent
    from potion.estimation.gradients import gpomdp_estimator
    from potion.estimation.importance_sampling import multiple_importance_weights, importance_weights
    
    env = gym.make('ContCartPole-v0')
    env.seed(0)
    seed_all_agent(0)
    N = 100
    H = 100
    disc = 0.99
    pol_b = Gauss(4,1, mu_init=[0.,0.,0.,0.], learn_std=True)    # Behavioural policy
    pol_t = Gauss(4,1, mu_init=[1.,1.,1.,1.], learn_std=True)    # Target policy
    
    batch = generate_batch(env, pol_b, H, N)
    
    # Compare on- and off-policy evaluations with a single policy
    # -----------------------------------------------------------
    for b in ['zero','peters']:
        on = gpomdp_estimator(batch, disc, pol_b, baselinekind=b, 
                            shallow=True)
        
        off = off_gpomdp_estimator(batch, disc, pol_b, pol_b.get_flat(), 
                                baselinekind=b,
                                shallow=True)

        on_samples = gpomdp_estimator(batch, disc, pol_b, baselinekind=b, shallow=True, result='samples')
        iws   = multiple_importance_weights(batch, pol_b, pol_b, 1)
        off_iws = torch.mean(torch.einsum('i,ij->ij', iws, on_samples), 0)

        print(f"{on}\n{off}\n{off_iws}")

    # Compare off-policy evaluations by means of importance weights external to the estimator and inside the estimator
    # ----------------------------------------------------------------------------------------------------------------
    import potion.algorithms.ce_optimization as ce_opt

    for b in ['zero','peters']:
        off1_samples = off_gpomdp_estimator(batch, disc, pol_b, pol_t.get_flat(),
                                           result='samples',
                                           baselinekind=b,
                                           shallow=True)
        off1 = torch.mean(off1_samples,0)
        off1_varmean = ce_opt.var_mean(off1_samples)[1]

        off2_samples = _shallow_multioff_gpomdp_estimator(batch, disc, pol_t, pol_b, 1,
                                                          result='samples',
                                                          baselinekind=b)
        off2 = torch.mean(off2_samples,0)
        off2_varmean = ce_opt.var_mean(off2_samples)[1]

        #NOTE: Here the only usable baseline should be zero, otherwise other baselines would not consider the contributions of the iws
        on_samples = gpomdp_estimator(batch, disc, pol_t, baselinekind=b, shallow=True, result='samples')
        iws   = multiple_importance_weights(batch, pol_t, pol_b, 1)
        # iws = importance_weights(batch, pol_b, pol_t.get_flat())
        off_iws = torch.mean(torch.einsum('i,ij->ij', iws, on_samples), 0)
        off_iws_varmean = ce_opt.var_mean(on_samples,iws)[1]

        print(f"Means:\n{off1}\n{off2}\n{off_iws}\n")
        print(f"Variances of means:\n{off1_varmean}\n{off2_varmean}\n{off_iws_varmean}\n")
    